

cls_num = 10

batch_size = 128
epoches = 80

# try this 1e-2 to 1e-3
# optimizer params
optimizer = 'adam'
lr_factor = 0.9
lr_steps = 2500
lr_stop_val = 1e-3
learning_rate = 1e-2
weight_decay = 1e-4

# The best now is: 1e-2 0.3 2500

generate_batches = 1


