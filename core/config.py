

cls_num = 10

batch_size = 128
epoches = 90

# try this 1e-2 to 1e-3
# optimizer params
optimizer = 'adam'
lr_factor = 0.7
lr_steps = 3000
lr_stop_val = 1e-4
learning_rate = 1e-2
weight_decay = 1e-4

# The best now is: 1e-2 1e-3 0.8 2500

generate_batches = 1


